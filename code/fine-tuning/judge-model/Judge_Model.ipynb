{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b28655-c18a-45ec-b4ff-a30ecd3e25ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.95.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Downloading openai-1.95.1-py3-none-any.whl (755 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [openai]2m1/2\u001b[0m [openai]\n",
      "Successfully installed jiter-0.10.0 openai-1.95.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9b0437c-e3f2-4f78-ba51-1b1d2c936725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e914c5-af2c-4b29-af8f-0b30c60de774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b501799-0234-41b6-8cb7-25ce837cb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c47f48e2-da5c-40e2-a6ca-8ce8b32723b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22ef97e6-f900-4b34-8acd-0fe4d19b826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate evaluation using Gemini\n",
    "def generate_evaluation(prompt):\n",
    "    import time\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=[prompt],\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.3,\n",
    "                    top_p=1.0,\n",
    "                    max_output_tokens=100,\n",
    "                    thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "                )\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if \"RESOURCE_EXHAUSTED\" in str(e) and attempt < max_retries - 1:\n",
    "                print(f\"Rate limit hit. Retrying after 45 seconds... (attempt {attempt+1})\")\n",
    "                time.sleep(45)\n",
    "            else:\n",
    "                print(f\"Error generating evaluation: {e}\")\n",
    "                return \"Score: [0]\"\n",
    "\n",
    "# CSV files to process\n",
    "csv_files = [\"tf_results.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a04e6e09-1121-4d4f-8016-144f298eaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Prepare a dictionary to store results for each model\n",
    "all_results = {}\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = f\"{file_name}\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(file_path).is_file():\n",
    "        print(f\"File not found: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Load the CSV for the current model\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Verify required columns\n",
    "    required_columns = [\"Generated Letter\", \"Reference Letter\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Missing required columns in {file_name}: {required_columns}\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare lists for storing evaluation results\n",
    "    scores = []\n",
    "\n",
    "    df_subset = df.head(2)\n",
    "    \n",
    "    for i, row in df_subset.iterrows():\n",
    "        generated_letter = row[\"Generated Letter\"]\n",
    "        reference_letter = row[\"Reference Letter\"]\n",
    "        \n",
    "        # Prepare the evaluation prompt\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the following Discharge Summaries:\n",
    "\n",
    "        Reference Discharge Summary:\n",
    "        {reference_letter}\n",
    "\n",
    "        Generated SOAP:\n",
    "        {generated_letter}\n",
    "\n",
    "        Rate the quality of the generated clinical letter on a scale of 0-10 based on the following criteria:\n",
    "        - Completeness: How much of the necessary information is included (0.25 weight)\n",
    "        - Correctness: Medical accuracy of the content (0.35 weight)\n",
    "        - Organization: Structure follows clinical letter format (0.20 weight)\n",
    "        - Clinical Relevance: Relevance of the content to clinical practice (0.20 weight)\n",
    "\n",
    "        Provide only the score from 0 to 10 based on the weighted evaluation.\n",
    "        Score: [ ]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate evaluation using Groq API\n",
    "        generated_content = generate_evaluation(prompt)\n",
    "        # print(f\"Row {i+1}/{len(df)} in {file_name}: {generated_content}\")\n",
    "\n",
    "        try:\n",
    "            # Extract score from the model's response\n",
    "            score_line = generated_content.split(\"Score:\")[1].split(\"\\n\")[0].strip()\n",
    "            score = float(score_line.replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse output at row {i+1} in {file_name}: {e}\")\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    # Save the judged results for the current model\n",
    "    df.loc[df_subset.index, \"Judge Score\"] = scores\n",
    "\n",
    "    # Store the evaluated dataframe in the dictionary\n",
    "    all_results[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cbbae38-c4fd-4263-ba86-d955c6b04151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for tf_results.csv completed and saved as judged_tf_results.csv!\n",
      "All evaluations completed!\n"
     ]
    }
   ],
   "source": [
    "# Save all judged results for each model\n",
    "for file_name, result_df in all_results.items():\n",
    "    output_file = f\"judged_{file_name}\"\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"Evaluation for {file_name} completed and saved as {output_file}!\")\n",
    "\n",
    "print(\"All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a3e9596-2421-4bff-90a4-c3d8f5ab25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Scores of each fine tuning technique\n",
    "facebook_bart_large_transfer_learning = pd.read_csv(\"judged_tf_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58ba5474-ab7e-49eb-9179-2cacfadbaf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.55)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean judge score\n",
    "facebook_bart_large_transfer_learning[\"Judge Score\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
