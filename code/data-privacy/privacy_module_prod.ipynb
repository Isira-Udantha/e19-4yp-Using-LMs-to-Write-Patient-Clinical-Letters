{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f39d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('ehr_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Presidio\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# Database configuration (replace with your PostgreSQL details)\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'ehr_db',\n",
    "    'user': 'secure_user',\n",
    "    'password': 'secure_password',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Encryption key (in production, use AWS KMS or similar)\n",
    "ENCRYPTION_KEY = get_random_bytes(32)  # AES-256 key\n",
    "NONCE = get_random_bytes(12)  # GCM nonce\n",
    "\n",
    "# Mock language model API (replace with actual model API)\n",
    "def mock_language_model(anonymized_text):\n",
    "    logger.info(\"Processing text with language model\")\n",
    "    return f\"Diagnosis: High risk of condition X for {anonymized_text}\"\n",
    "\n",
    "# Database setup\n",
    "def setup_database():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS phi_mapping (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                record_id VARCHAR(50),\n",
    "                token VARCHAR(50),\n",
    "                encrypted_phi TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        logger.info(\"Database table created or verified\")\n",
    "        return conn, cursor\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Encrypt PHI\n",
    "def encrypt_phi(phi, key, nonce):\n",
    "    cipher = AES.new(key, AES.MODE_GCM, nonce=nonce)\n",
    "    ciphertext, tag = cipher.encrypt_and_digest(phi.encode('utf-8'))\n",
    "    return base64.b64encode(nonce + ciphertext + tag).decode('utf-8')\n",
    "\n",
    "# Decrypt PHI\n",
    "def decrypt_phi(encrypted_phi, key):\n",
    "    try:\n",
    "        raw = base64.b64decode(encrypted_phi)\n",
    "        nonce, ciphertext, tag = raw[:12], raw[12:-16], raw[-16:]\n",
    "        cipher = AES.new(key, AES.MODE_GCM, nonce=nonce)\n",
    "        decrypted = cipher.decrypt_and_verify(ciphertext, tag)\n",
    "        return decrypted.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Decryption failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Anonymize single record\n",
    "def anonymize_record(record, record_id, cursor):\n",
    "    try:\n",
    "        text = record.get('clinical_notes', '')\n",
    "        if not text:\n",
    "            return None, None\n",
    "\n",
    "        # Detect PHI\n",
    "        results = analyzer.analyze(text=text, entities=[\"PERSON\", \"DATE_TIME\", \"PHONE_NUMBER\", \"MEDICAL_RECORD\"], language=\"en\")\n",
    "        \n",
    "        # Create mapping\n",
    "        mapping = {}\n",
    "        counter = 1\n",
    "        for result in results:\n",
    "            original = text[result.start:result.end]\n",
    "            token = f\"{result.entity_type}_{record_id}_{counter:03d}\"\n",
    "            mapping[original] = token\n",
    "            encrypted_phi = encrypt_phi(original, ENCRYPTION_KEY, NONCE)\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO phi_mapping (record_id, token, encrypted_phi) VALUES (%s, %s, %s)\",\n",
    "                (record_id, token, encrypted_phi)\n",
    "            )\n",
    "            counter += 1\n",
    "        \n",
    "        # Anonymize text\n",
    "        anonymized_result = anonymizer.anonymize(\n",
    "            text=text,\n",
    "            analyzer_results=results,\n",
    "            operators={entity: {\"type\": \"replace\", \"new_value\": mapping.get(text[result.start:result.end], f\"{entity}_{record_id}_{counter:03d}\")}\n",
    "                       for entity, result in [(r.entity_type, r) for r in results]}\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Anonymized record {record_id}\")\n",
    "        return anonymized_result.text, mapping\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Anonymization failed for record {record_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Process batch of records\n",
    "def process_batch(batch, cursor):\n",
    "    results = []\n",
    "    for index, record in batch.iterrows():\n",
    "        record_id = str(record.get('patient_id', index))\n",
    "        anonymized_text, mapping = anonymize_record(record, record_id, cursor)\n",
    "        if anonymized_text:\n",
    "            results.append((record_id, anonymized_text, mapping))\n",
    "    return results\n",
    "\n",
    "# Deanonymize model output\n",
    "def deanonymize_output(record_id, anonymized_text, cursor):\n",
    "    try:\n",
    "        cursor.execute(\"SELECT token, encrypted_phi FROM phi_mapping WHERE record_id = %s\", (record_id,))\n",
    "        mapping = {decrypt_phi(row[1], ENCRYPTION_KEY): row[0] for row in cursor.fetchall()}\n",
    "        \n",
    "        deanonymized_text = anonymized_text\n",
    "        for original, token in mapping.items():\n",
    "            deanonymized_text = deanonymized_text.replace(token, original)\n",
    "        \n",
    "        logger.info(f\"Deanonymized record {record_id}\")\n",
    "        return deanonymized_text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Deanonymization failed for record {record_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Main pipeline\n",
    "def ehr_anonymization_pipeline(input_file, output_file):\n",
    "    try:\n",
    "        # Setup database\n",
    "        conn, cursor = setup_database()\n",
    "        \n",
    "        # Load EHR data\n",
    "        df = pd.read_csv(input_file)\n",
    "        logger.info(f\"Loaded {len(df)} records from {input_file}\")\n",
    "        \n",
    "        # Batch processing with multiprocessing\n",
    "        batch_size = 100\n",
    "        batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "        results = []\n",
    "        \n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            for batch_results in executor.map(partial(process_batch, cursor=cursor), batches):\n",
    "                results.extend(batch_results)\n",
    "        \n",
    "        conn.commit()  # Commit anonymized mappings\n",
    "        \n",
    "        # Process with language model and deanonymize\n",
    "        output_data = []\n",
    "        for record_id, anonymized_text, _ in results:\n",
    "            model_output = mock_language_model(anonymized_text)\n",
    "            deanonymized_output = deanonymize_output(record_id, model_output, cursor)\n",
    "            output_data.append({'record_id': record_id, 'output': deanonymized_output})\n",
    "        \n",
    "        # Save results\n",
    "        output_df = pd.DataFrame(output_data)\n",
    "        output_df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Saved results to {output_file}\")\n",
    "        \n",
    "        # Clean up\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return output_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample CSV: columns = ['patient_id', 'clinical_notes']\n",
    "    sample_data = pd.DataFrame({\n",
    "        'patient_id': ['P001', 'P002'],\n",
    "        'clinical_notes': [\n",
    "            \"Patient John Doe was admitted on 2025-07-12 with phone 555-123-4567.\",\n",
    "            \"Patient Jane Smith visited on 2025-07-10, MRN 123456.\"\n",
    "        ]\n",
    "    })\n",
    "    sample_data.to_csv('ehr_input.csv', index=False)\n",
    "    \n",
    "    ehr_anonymization_pipeline('ehr_input.csv', 'ehr_output.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
