{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbEjYDKINkB"
      },
      "source": [
        "RAG Pipeline to Check Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfC16xfQbRxF"
      },
      "source": [
        "1. Preprocess and Chunk the EHR Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YUb3cMvnbaO9"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import re\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500) -> List[str]:\n",
        "    \"\"\"Split EHR text into manageable chunks\"\"\"\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sum(len(s) for s in current_chunk) + len(sentence) < chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7T4w4XddY0u"
      },
      "source": [
        "2. Embed and Store Chunks in Vector DB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u6XCu5ceB5d",
        "outputId": "e9167d64-98f3-4cce-8e6b-8ad721843ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Lg30TVWRdj-l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prage\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prage\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_chunks(chunks: List[str]):\n",
        "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
        "    return embeddings\n",
        "\n",
        "def store_in_faiss(embeddings, chunks):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS7xiRSWss0j"
      },
      "source": [
        "3. Extract Claims from Generated Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2R3Ijmt0sybF"
      },
      "outputs": [],
      "source": [
        "def extract_claims(summary: str) -> List[str]:\n",
        "    return re.split(r'(?<=[.?!])\\s+', summary.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BthifMBtQAs"
      },
      "source": [
        "4. Retrieve Top-K EHR Chunks for Each Claim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "W0skclhhtQ87"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(claims: List[str], index, chunks: List[str], k: int = 3):\n",
        "    claim_embeddings = model.encode(claims, convert_to_numpy=True)\n",
        "    D, I = index.search(claim_embeddings, k)\n",
        "\n",
        "    retrieved = []\n",
        "    for idx_list in I:\n",
        "        retrieved.append([chunks[i] for i in idx_list])\n",
        "\n",
        "    return retrieved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAwRR_4ItV9i"
      },
      "source": [
        "5. Verify Claim Support (LLM-based or Rule-based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VJrri1jEvmJE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\".env\") \n",
        "\n",
        "api_key = os.getenv(\"NEBIUS_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"API key is missing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m38oMm2ctZ8b"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
        "    api_key=api_key,\n",
        ")\n",
        "\n",
        "def verify_claim(claim: str, evidence: list) -> str:\n",
        "    context = \"\\n\\n\".join(evidence)\n",
        "    prompt = f\"\"\"\n",
        "    Given the following EHR context:\n",
        "    {context}\n",
        "\n",
        "    Does this context support the claim: \"{claim}\"?\n",
        "    Answer with: Supported / Not Supported / Uncertain and a brief reason.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-Coder-7B\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzuEAg9CuMoa",
        "outputId": "e15a2ef0-608a-43b4-9521-35e5a1de4b50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Create the patient record (EHR)\n",
        "ehr_text = \"\"\"\n",
        "Patient was admitted on 2023-04-02 with fever, hypotension, and positive blood cultures.\n",
        "Blood cultures confirmed methicillin-resistant Staphylococcus aureus (MRSA).\n",
        "The patient was treated with intravenous vancomycin for 5 days and improved clinically.\n",
        "Discharge diagnosis was sepsis due to MRSA.\n",
        "\"\"\"\n",
        "\n",
        "# Create the generated discharge summary\n",
        "summary = \"\"\"\n",
        "The patient was admitted for sepsis due to MRSA and received intravenous vancomycin for 5 days.\n",
        "\"\"\"\n",
        "\n",
        "# Write to files\n",
        "with open(\"patient_record.txt\", \"w\") as f:\n",
        "    f.write(ehr_text)\n",
        "\n",
        "with open(\"generated_summary.txt\", \"w\") as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"Files created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LQWjBmltnj7",
        "outputId": "85b4425c-4b63-462a-ba50-dcdecbda7308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claim: The patient was admitted for sepsis due to MRSA and received intravenous vancomycin for 5 days.\n",
            "Verdict: Supported. The context clearly states that the patient was admitted for sepsis due to MRSA and received intravenous vancomycin for 5 days.\n",
            "\n",
            "    Given the following EHR context:\n",
            "    \n",
            "Patient was admitted on 2023-04-02 with fever, hypotension, and positive blood cultures. Blood cultures confirmed methicillin-resistant Staphylococcus aureus (MRSA). The patient was treated with intravenous vancomycin for 5 days and improved clinically. Discharge diagnosis was sepsis due to MRSA. \n",
            "\n",
            "\n",
            "Patient was admitted on 2023-04-02 with fever, hypotension, and positive blood cultures. Blood cultures confirmed methicillin-resistant Staphylococcus aureus (MRSA). The patient was treated with intravenous vancomycin for 5 days and improved clinically. Discharge diagnosis was sepsis due to MRSA. \n",
            "\n",
            "\n",
            "Patient was admitted on 2023-04-02 with fever, hypotension, and positive blood cultures. Blood cultures confirmed methicillin-resistant Staphylococcus aureus (MRSA). The patient was treated with intravenous vancomycin for 5 days and improved clinically. Discharge diagnosis was sepsis due to MRSA. \n",
            "\n",
            "    Does this context support the claim: \"The patient was admitted for sepsis due to MRSA and received intravenous vancomycin for 5 days.\"?\n",
            "    Answer with: Supported / Not Supported / Uncertain and a brief reason.\n",
            "    <|fim_middle|>\n",
            "<|file_sep|><|fim_prefix|>/README.md\n",
            "# Qwen-LLM-Chatbot\n",
            "This is a simple chatbot based on Qwen-LLM, a large language model developed by Alibaba Cloud. The chatbot is designed to provide a conversational interface for users to interact with the model.\n",
            "\n",
            "## Getting Started\n",
            "\n",
            "To get started with the chatbot, you will need to have Python 3.6 or later installed on your system. You will also need to install the following dependencies:\n",
            "\n",
            "```\n",
            "pip install transformers\n",
            "pip install torch\n",
            "pip install flask\n",
            "```\n",
            "\n",
            "Once you have installed the dependencies, you can run the chatbot by executing the following command:\n",
            "\n",
            "```\n",
            "python chatbot.py\n",
            "```\n",
            "\n",
            "This will start the chatbot server on port 5000. You can then interact with the chatbot by sending HTTP POST requests to the `/chat` endpoint with the following JSON payload:\n",
            "\n",
            "```\n",
            "{\n",
            "    \"message\": \"Hello, how are you?\"\n",
            "}\n",
            "```\n",
            "\n",
            "The chatbot will respond with a JSON object containing the model's response:\n",
            "\n",
            "```\n",
            "{\n",
            "    \"response\": \"I'm doing well, thank you. How can I help you today?\"\n",
            "}\n",
            "```\n",
            "\n",
            "## Customizing the Chatbot\n",
            "\n",
            "You can customize the chatbot by modifying the `chatbot.py` file. The `chatbot.py` file contains the following functions:\n",
            "\n",
            "- `get_response(message)`: This function takes a message as input and returns the model's response.\n",
            "- `get_chatbot_response(message)`: This function takes a message as input and returns a JSON object containing the model's response.\n",
            "- `app.run()`: This function starts the chatbot server on port 5000.\n",
            "\n",
            "You can modify the `get_response` function to customize the model's behavior. For example, you can modify the function to include additional context or to use a different model.\n",
            "\n",
            "You can also modify the `get_chatbot_response` function to customize the JSON response format. For example, you can modify the function to include additional information about the model's response.\n",
            "\n",
            "## Deploying the Chatbot\n",
            "\n",
            "You can deploy the chatbot on a cloud platform such as AWS or Google Cloud. To do this, you will need to create a Docker image of the chatbot and deploy it to the cloud platform.\n",
            "\n",
            "To create a Docker image of the chatbot, you can use the following command:\n",
            "\n",
            "```\n",
            "docker build -t chatbot .\n",
            "```\n",
            "\n",
            "This will create a Docker image named `chatbot` based on the current directory.\n",
            "\n",
            "To deploy the Docker image to a cloud platform, you will need to push the image to a container registry such as Docker Hub or Amazon ECR. You can then deploy the image to the cloud platform using the container registry.\n",
            "\n",
            "Once the chatbot is deployed, you can interact with it by sending HTTP POST requests to the `/chat` endpoint with the following JSON payload:\n",
            "\n",
            "```\n",
            "{\n",
            "    \"message\": \"Hello, how are you?\"\n",
            "}\n",
            "```\n",
            "\n",
            "The chatbot will respond with a JSON object containing the model's response:\n",
            "\n",
            "```\n",
            "{\n",
            "    \"response\": \"I'm doing well, thank you. How can I help you today?\"\n",
            "}\n",
            "```\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "This is a simple chatbot based on Qwen-LLM, a large language model developed by Alibaba Cloud. The chatbot is designed to provide a conversational interface for users to interact with the model. You can customize the chatbot by modifying the `chatbot.py` file and deploy it on a cloud platform such as AWS or Google Cloud.\n",
            "\n",
            "## License\n",
            "\n",
            "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
            "\n",
            "## Acknowledgments\n",
            "\n",
            "- [Alibaba Cloud](https://www.alibabacloud.com/)\n",
            "- [Qwen-LLM](https://qwen.alibabacloud.com/)\n",
            "- [Transformers](https://huggingface.co/transformers/)\n",
            "- [Flask](https://flask.palletsprojects.com/)\n",
            "- [Docker](https://www.docker.com/)\n",
            "- [AWS](https://aws.amazon.com/)\n",
            "- [Google Cloud](https://cloud.google.com/)\n",
            "- [Docker Hub](https://hub.docker.com/)\n",
            "- [Amazon ECR](https://aws.amazon.com/ecr/)\n",
            "- [MIT License](https://opensource.org/licenses/MIT)\n",
            "- [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
            "- [BSD License](https://opensource.org/licenses/BSD-3-Clause)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [MPL License](https://www.mozilla.org/en-US/MPL/2.0/)\n",
            "- [EPL License](https://www.eclipse.org/legal/epl-2.0/)\n",
            "- [AGPL License](https://www.gnu.org/licenses/agpl-3.0.en.html)\n",
            "- [GPL License](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
            "- [LGPL License](https://www.gnu.org/licenses/lgpl-3.0.en.html)\n",
            "- [M\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ehr_text = open(\"patient_record.txt\").read()\n",
        "summary = open(\"generated_summary.txt\").read()\n",
        "\n",
        "chunks = chunk_text(ehr_text)\n",
        "chunk_embeddings = embed_chunks(chunks)\n",
        "index = store_in_faiss(chunk_embeddings, chunks)\n",
        "\n",
        "claims = extract_claims(summary)\n",
        "retrieved_chunks = retrieve_chunks(claims, index, chunks)\n",
        "\n",
        "for claim, evidence in zip(claims, retrieved_chunks):\n",
        "    verdict = verify_claim(claim, evidence)\n",
        "    print(f\"Claim: {claim}\\nVerdict: {verdict}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
